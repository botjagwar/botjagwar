"""
This script reads SQLite database file and updates the local one as it goes.

The SQLite database MUST be a Base dictionary generated by botjagwar (see the code at
database/dictionary/__init__.py if you want to know what I'm talking about), or else DictionaryDatabaseManager
supposed to handle both the local and the dictionary-to-be-imported will be lost.

if update_on_wiki is set to True, it also creates entries on-wiki if the latter does not exist on the Malagasy
Wiktionary.
"""

from pprint import pprint

import pywikibot

from api.databasemanager import DictionaryDatabaseManager
from api.decorator import time_this
from api.output import Output
from database.dictionary import Word
from object_model.word import Entry

# To build a really fast tree to avoid IO-expensive table lookups
output_database = DictionaryDatabaseManager()
fast_tree = set()

# Sometimes Pywikibot API will do nothing but slow us down
site = pywikibot.Site('mg', 'wiktionary')
update_on_wiki = True

# Path where we should expect to find the exported dictionary
export_path = 'user_data/dump_export.db'


@time_this('Fast tree building')
def build_fast_tree():
    """
    Table lookups are IO-expensive, so build a fast lookup tree to check entry existence in database
    :return:
    """
    print("--- building fast tree ---")
    q = output_database.session.query(Word)
    for w in q.yield_per(1000):
        fast_tree.add((w.word, w.language))
    print('out fast tree contains %d items' % len(fast_tree))
    print("--- done building fast tree ---")


@time_this('fast tree lookup')
def lookup(entry, language):
    if (entry, language) in fast_tree:
        return True
    else:
        return False


@time_this('entry worker')
def worker(entry: Entry):
    """
    Updates the wiki page with the given entry.
    If entry exists in database, skip;
    else, check language's existence on-wiki and if it exists, skip;
    else, add the entry on-wiki
    :param entry: entry to create
    :return:
    """
    pprint(entry)
    if lookup(entry.entry, entry.language):
        print('fast tree lookup: already on database')
        return
    else:
        output = Output()
        output.db(entry)

    if not update_on_wiki:
        print('not updating on wiki')
        return

    print('attempts to update on wiki...')
    wikipage = output.wikipage(entry)
    page = pywikibot.Page(site, entry.entry)
    if page.exists() and not page.isRedirectPage():
        content = page.get()
        if '{{=%s=}}' % entry.language in content:
            print('exists on-wiki')
            return
        else:
            content = wikipage + content
    else:
        content = wikipage

    page.put(content, "dikan-teny avy amin'ny tahiry XML")


def import_database(workers=100):
    """
    Imports database.
    Entry creation could be heavily parallelised but a some point Pywikibot will
    be a serious bottleneck.

    :param workers: Number of items to yield; initially intended to parallelise the work
    but Pywikibot API won't let us flood Wikimedia sites :P
    :return:
    """
    input_database = DictionaryDatabaseManager(database_file=export_path)
    query = input_database.session.query(Word) \
        .order_by(Word.id.desc())

    for element in query.yield_per(workers):
        worker(element.serialise_to_entry())


if __name__ == '__main__':
    build_fast_tree()
    import_database()
