"""
This script reads SQLite database file and updates the local one as it goes.

The SQLite database MUST be a Base dictionary generated by botjagwar (see the code at
database/dictionary/__init__.py if you want to know what I'm talking about), or else DictionaryDatabaseManager
supposed to handle both the local and the dictionary-to-be-imported will be lost.

if update_on_wiki is set to True, it also creates entries on-wiki if the latter does not exist on the Malagasy
Wiktionary.
"""

from pprint import pprint

import pywikibot

from api.data_caching import FastWordLookup
from api.databasemanager import DictionaryDatabaseManager
from api.decorator import time_this
from api.output import Output
from object_model.word import Entry

# To build a really fast tree to avoid IO-expensive table lookups
fast_tree = set()

# Sometimes Pywikibot API will do nothing but slow us down
site = pywikibot.Site('mg', 'wiktionary')
update_on_wiki = True

# Path where we should expect to find the exported dictionary
export_path = 'user_data/dump_export.db'

# Fast word lookup table
lookup_cache = FastWordLookup()
lookup_cache.build_fast_word_tree()


@time_this('entry worker')
def worker(entry: Entry):
    """
    Updates the wiki page with the given entry.
    If entry exists in database, skip;
    else, check language's existence on-wiki and if it exists, skip;
    else, add the entry on-wiki
    :param entry: entry to create
    :return:
    """
    pprint(entry)
    if lookup_cache.lookup(entry):
        print('fast tree lookup: already on database')
        return
    else:
        output = Output()
        output.db(entry)

    if not update_on_wiki:
        print('not updating on wiki')
        return

    print('attempts to update on wiki...')
    wikipage = output.wikipage(entry)
    page = pywikibot.Page(site, entry.entry)
    if page.exists() and not page.isRedirectPage():
        content = page.get()
        if '{{=%s=}}' % entry.language in content:
            print('exists on-wiki')
            return
        else:
            content = wikipage + '\n' + content
    else:
        content = wikipage

    page.put(content, "dikan-teny avy amin'ny tahiry XML")


def import_database(workers=100):
    """
    Imports database.
    Entry creation could be heavily parallelised but a some point Pywikibot will
    be a serious bottleneck.

    :param workers: Number of items to yield; initially intended to parallelise the work
    but Pywikibot API won't let us flood Wikimedia sites :P
    :return:
    """
    fast_tree = {}
    input_database = DictionaryDatabaseManager(database_file=export_path)
    with input_database.engine.connect() as connection:
        query = connection.execute(
            """
            select 
                word.id,
                word.word, 
                word.language, 
                word.part_of_speech, 
                definitions.definition,
                definitions.definition_language
            from 
                dictionary,
                word, 
                definitions
            where 
                dictionary.definition = definitions.id
                and word.id = dictionary.word
                and definition_language = 'mg'
            """
        )
        print('-- build tree --')
        for w in query.fetchall():
            word, language, part_of_speech, definition = w[1], w[2], w[3], w[4]
            key = (word, language, part_of_speech)
            if key in fast_tree:
                fast_tree[key].append(definition)
            else:
                fast_tree[key] = [definition]

        print('-- using tree --')
        for word, language, part_of_speech in fast_tree:
            entry = Entry(
                entry=word,
                language=language,
                part_of_speech=part_of_speech,
                entry_definition=fast_tree[(word, language, part_of_speech)]
            )
            worker(entry)


if __name__ == '__main__':
    import_database()
